run:
  reduce_outliers: false

model:
  bertopic:
    min_topic_size: 80
    language: english
    top_n_words: 15
    nr_topics: auto
    calculate_probabilities: true

  sentence_transformer: allenai-specter # all-mpnet-base-v2 # all-MiniLM-L6-v2 # allenai-specter
  # sentence_transformer: "paraphrase-multilingual-MiniLM-L12-v2"

  dimensionality_reduction:
    choice: umap
    params:
      umap:
        n_neighbors: 80
        n_components: 5 # the higher, the more "spiky"
        min_dist: 0.0
        metric: cosine
        low_memory: false
        random_state: 654565

  clustering:
    choice: gmm # hdbscan
    params:
      hdbscan:
        min_cluster_size: 10
        min_samples: 10
        metric: euclidean
        cluster_selection_method: eom
        prediction_data: true
      kmeans:
        n_clusters: 2
      gmm:
        n_components: 2
        init_params: k-means++
        random_state: 47

  vectorizer:
    params:
      stop_words: english
      ngram_range: !!python/tuple [ 1, 2 ]
      #      max_df: 0.8
      #      min_df: 0.2
      max_features: 10000


  weighting:
    params:
      bm25_weighting: true
      reduce_frequent_words: true


  representation:
    choice: [ keybert, mmr ] # keybert, mmr, pos
    params:
      keybert:
        random_state: 17
        top_n_words: 15
      mmr:
        diversity: 0.3
        top_n_words: 15
      pos:
        model: en_core_web_lg
        top_n_words: 15
        pos_patterns: [
          [ { 'POS': 'ADJ' }, { 'POS': 'NOUN' } ],
          [ { 'POS': 'NOUN' } ],
          [ { 'POS': 'ADJ' } ]
        ]


